#!/bin/bash

# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify it under the
# terms of the GNU Affero General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
# PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License along
# with this program. If not, see <https://www.gnu.org/licenses/>. For any questions
# about this software or licensing, please email opensource@seagate.com or
# cortx-questions@seagate.com.

set -eu -o pipefail
export PS4='+ [${BASH_SOURCE[0]##*/}:${LINENO}${FUNCNAME[0]:+:${FUNCNAME[0]}}] '
# set -x

PROG=${0##*/}


log_file=${BUILD_HA_CSM_LOG_FILE:-/var/log/cortx-ha.log}
exec &>> $log_file
exec &> >(stdbuf -oL gawk '{ print strftime("%Y-%m-%d %H:%M:%S"), $0 }')


usage() {
    cat <<EOF
Usage: $PROG [OPTS] [<params.yaml>]

Configures CSM HA by preparing the configuration files and
adding resources into the Pacemaker.

Caveats:

* The script expects Pacemaker to be started and have no resources configured.
  Check with 'pcs status'.

* Passwordless SSH access between the nodes is required.

* The script should be executed from the "left" node.

* Ensure that the provided roaming IP address belongs to
  the management network interface and local subnetwork, which
  are not used.

* Consul should be started on all cluster nodes.

* Elastic search should be started on all cluster nodes.

Mandatory parameters:
  --vip <addr>          CSM roaming IP address
  -i, --interface <if>  Management network interface (default: eth1)
  --left-node     <n1>  Left node hostname (default: pod-c1)
  --right-node    <n2>  Right node hostname (default: pod-c2)

Optional parameters:
  --cib-file            Pacemaker configuration file.
  --update              Preserve Consul and Motr state, reconfigure Pacemaker only.

Note: parameters can be specified either directly via command line options
or via YAML file, e.g.:

  vip: <vip>
  interface: <iface>
  left-node: <lnode>
  right-node: <rnode>
EOF
}

log() {
    logger --stderr --tag build-ha-csm "$*"
}
log "****************************************************************"
log "${0##*/}"
log "****************************************************************"



TEMP=$(getopt --options h,i: \
              --longoptions help,vip:,interface:,left-node:,right-node: \
              --longoptions cib-file:,update \
              --name "$PROG" -- "$@" || true)

(($? == 0)) || { usage >&2; exit 1; }

eval set -- "$TEMP"

vip=
iface=eth1
lnode=pod-c1
rnode=pod-c2
update=false
cib_file=/var/lib/hare/cib_cortx_cluster.xml

while true; do
    case "$1" in
        -h|--help)           usage; exit ;;
        --vip)               vip=$2; shift 2 ;;
        -i|--interface)      iface=$2; shift 2 ;;
        --left-node)         lnode=$2; shift 2 ;;
        --right-node)        rnode=$2; shift 2 ;;
        --cib-file)          cib_file=$2; shift 2 ;;
        --update)            update=true; shift 2 ;;
        --)                  shift; break ;;
        *)                   break ;;
    esac
done

argsfile=${1:-}

if [[ -f $argsfile ]]; then
    while IFS=': ' read name value || [[ $name =~ [a-zA-Z0-9] ]]; do
       case $name in
           vip)          vip=$value     ;;
           interface)    iface=$value   ;;
           left-node)    lnode=$value   ;;
           right-node)   rnode=$value   ;;
           '')                          ;;
           *) echo "Invalid parameter '$name' in $argsfile" >&2
              usage >&2; exit 1 ;;
       esac
    done < $argsfile
fi

[[ $vip ]] || {
    usage >&2
    exit 1
}

die() {
    echo "[$HOSTNAME] $PROG: $*" >&2
    exit 1
}

run_on_both() {
    local cmd=$*
    eval $cmd
    ssh $rnode $cmd
}

precheck() {
    log "${FUNCNAME[0]}: Running precheck"
    systemctl is-active --quiet hare-consul-agent-c1 ||
        die 'No active Consul instance found'
    ssh $rnode "systemctl is-active --quiet hare-consul-agent-c2" ||
        die 'No active Consul instance found'

    systemctl is-active --quiet elasticsearch ||
        die 'No active elasticsearch instance found'
    ssh $rnode "systemctl is-active --quiet elasticsearch" ||
        die 'No active elasticsearch instance found'

    systemctl is-active --quiet rabbitmq-server ||
        die 'No active rabbitmq instance found'
    ssh $rnode "systemctl is-active --quiet rabbitmq-server" ||
        die 'No active rabbitmq instance found'
}

systemd_disable() {
    echo 'Disabling csm and kibana systemd units...'
    log "${FUNCNAME[0]}: Disabling csm and kibana systemd units"
    units_to_disable=(
        kibana
        csm_web
        csm_agent
    )

    for u in ${units_to_disable[@]}; do
        run_on_both "sudo systemctl stop $u && sudo systemctl disable $u"
    done
}

kibana_rsc_add() {
    echo 'Adding kibana resources...'
    log "${FUNCNAME[0]}: Adding kibana resources"
    sudo pcs -f $cib_file resource create kibana-vip ocf:heartbeat:IPaddr2 \
        ip=$vip cidr_netmask=24 nic=$iface iflabel=v1 \
        op start   interval=0s timeout=60s \
        op monitor interval=5s timeout=20s \
        op stop    interval=0s timeout=60s
    sudo pcs -f $cib_file resource create kibana systemd:kibana op monitor interval=30s
}

csm_rsc_add() {
    echo 'Adding csm resources and constraints...'
    log "${FUNCNAME[0]}: Adding csm resources and constraints"

    sudo pcs -f $cib_file resource create csm-agent systemd:csm_agent op \
        monitor interval=30s
    sudo pcs -f $cib_file resource create csm-web systemd:csm_web op \
        monitor interval=30s

    sudo pcs -f $cib_file resource group add \
        csm-kibana kibana-vip kibana csm-web csm-agent

    sudo pcs -f $cib_file constraint order consul-c1 then csm-web
    sudo pcs -f $cib_file constraint order consul-c2 then csm-web
    sudo pcs -f $cib_file constraint order els-search-clone then csm-kibana

    sudo pcs -f $cib_file constraint location csm-kibana prefers $lnode

    sudo pcs -f $cib_file constraint location csm-kibana rule score=-INFINITY \
        '#uname' eq $lnode and consul-c1-running eq 0
    sudo pcs -f $cib_file constraint location csm-kibana rule score=-INFINITY \
        '#uname' eq $rnode and consul-c2-running eq 0

    sudo pcs -f $cib_file constraint colocation add csm-kibana with els-search-clone \
        score=INFINITY
    sudo pcs -f $cib_file constraint colocation add csm-kibana with rabbitmq-clone \
        score=INFINITY
}

cib_init() {
   sudo pcs cluster cib $cib_file
}

cib_commit() {
    sudo pcs cluster cib-push $cib_file --config
}

# HA operations table.
ha_ops=(
    precheck
    systemd_disable
    kibana_rsc_add
    csm_rsc_add
)


# Maps ha operation from the ha_ops table to its respective type.
# HA operations are classified and described as follows,
# bootstrap:   executes during clean installation of the software only
# update:      executes during clean install and software update
declare -A ha_ops_type=(
    [precheck]='bootstrap'
    [systemd_disable]='bootstrap'
    [kibana_rsc_add]='update'
    [csm_rsc_add]='update'
)

for op in ${ha_ops[@]}; do
    if ! $update; then
        cib_init
        $op
        cib_commit
    elif [[ ${ha_ops_type[$op]} == 'update' ]]; then
        # We are using existing CIB as a base and re-applying the pcs
        # instructions, thus some instructions would already exist in the
        # CIB, we ignore them.
        $op || true
    fi
    log "CSM resource addition done"
done
